{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e66cc0c1",
   "metadata": {},
   "source": [
    "# Indonesian Document Retrieval System\n",
    "**Objective:** Build an explainable document retrieval system (Indonesian) using ECS for stemming and VSM with TF-IDF and Cosine Similarity.  \n",
    "This notebook demonstrates each preprocessing step, shows tables and intermediate outputs, and ranks documents for a sample query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575aa3ea",
   "metadata": {},
   "source": [
    "## Libraries used\n",
    "- `os`, `glob` : file handling\n",
    "- `PyPDF2`, `python-docx` : read `.pdf` and `.docx`\n",
    "- `re` : tokenization\n",
    "- `pandas`, `numpy`: tables and math\n",
    "- `math` : log in IDF\n",
    "- `sklearn.metrics.pairwise` (optional) : cosine similarity comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7663c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, re, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "# pdf/docx readers\n",
    "import PyPDF2\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82ae31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 documents. Sample filenames:\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"dataset\"  # update if needed\n",
    "\n",
    "def read_txt(path):\n",
    "    with open(path, encoding='utf-8', errors='ignore') as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(path):\n",
    "    text = []\n",
    "    with open(path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text.append(page.extract_text() or \"\")\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def read_docx(path):\n",
    "    doc = docx.Document(path)\n",
    "    return \"\\n\".join(p.text for p in doc.paragraphs)\n",
    "\n",
    "def load_documents(folder):\n",
    "    readers = {'txt': read_txt, 'pdf': read_pdf, 'docx': read_docx}\n",
    "    docs = {}\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for fname in files:\n",
    "            ext = fname.rsplit('.', 1)[-1].lower()\n",
    "            if ext in readers:\n",
    "                path = os.path.join(root, fname)\n",
    "                try:\n",
    "                    docs[os.path.relpath(path, folder)] = readers[ext](path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed reading {path}: {e}\")\n",
    "    return docs\n",
    "\n",
    "# Load\n",
    "documents = load_documents(DATA_DIR)\n",
    "print(f\"Loaded {len(documents)} documents. Sample filenames:\")\n",
    "for i, name in enumerate(list(documents)[:10], 1):\n",
    "    print(i, name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e86d6be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: d:\\Semester 5\\Data Mining\\tubes\n",
      "DATA_DIR (abs): d:\\Semester 5\\Data Mining\\tubes\\dataset\n",
      "DATA_DIR exists?: True\n",
      "List root dataset folder: ['doc', 'pdf', 'texts']\n",
      "Glob txt in DATA_DIR: []\n",
      "Glob all files recursively: ['dataset\\\\doc\\\\1.docx', 'dataset\\\\doc\\\\241-SO-T6.docx', 'dataset\\\\doc\\\\2_Tugas_2-IFB354-1520193.docx', 'dataset\\\\doc\\\\2_Tugas_2-IFB354-1520193[1].docx', 'dataset\\\\doc\\\\AAA.docx', 'dataset\\\\doc\\\\Alamat akhir ram.docx', 'dataset\\\\doc\\\\Analisis Data Indeks SPBE Menggunakan OLAP untuk Evaluasi Kinerja Digitalisasi di Kabupaten.docx', 'dataset\\\\doc\\\\Api.docx', 'dataset\\\\doc\\\\BAB 1 PENDAHULUAN.docx', 'dataset\\\\doc\\\\basis data 2.docx', 'dataset\\\\doc\\\\basisdata_chap7.docx', 'dataset\\\\doc\\\\basisdata_chap8.docx', 'dataset\\\\doc\\\\CHAPTER 4_BASIS DATA.docx', 'dataset\\\\doc\\\\ciporeat.docx', 'dataset\\\\doc\\\\Cover Laporan Prak OOP 2025 (2).docx', 'dataset\\\\doc\\\\Doc1.docx', 'dataset\\\\doc\\\\Doc2.docx', 'dataset\\\\doc\\\\Dokumentasi Kuis Konfigurasi Jaringan VLAN.docx', 'dataset\\\\doc\\\\Dokumentasi Tugas acl.docx', 'dataset\\\\doc\\\\Dokumentasi UTS Konfigurasi Jaringan VLAN.docx']\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "print(\"cwd:\", os.getcwd())\n",
    "print(\"DATA_DIR (abs):\", os.path.abspath(DATA_DIR))\n",
    "print(\"DATA_DIR exists?:\", os.path.exists(DATA_DIR))\n",
    "print(\"List root dataset folder:\", os.listdir(\"dataset\"))\n",
    "print(\"Glob txt in DATA_DIR:\", glob.glob(os.path.join(DATA_DIR, \"*.txt\"))[:10])\n",
    "print(\"Glob all files recursively:\", glob.glob(os.path.join(\"dataset\", \"**\", \"*.*\"), recursive=True)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5a819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name, sample_text = next(iter(documents.items()))\n",
    "print(\"Original (first 300 chars):\\n\", sample_text[:300])\n",
    "print(\"\\nLowercased:\\n\", sample_text[:300].lower()[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f64104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    # remove non-alphanumeric (keep Indonesian letters)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    tokens = [t for t in text.split() if t.strip()]\n",
    "    return tokens\n",
    "\n",
    "tokens_example = tokenize(sample_text)[:50]\n",
    "print(\"First 50 tokens:\", tokens_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343922bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDONESIAN_STOPWORDS = {\n",
    "    # condensed sample list; extend as needed\n",
    "    'yang','dan','di','ke','dari','ini','itu','pada','adalah','sebagai',\n",
    "    'dengan','atau','oleh','untuk','karena','saat','kami','saya','kamu',\n",
    "    'adanya','sebagai','telah','akan','adalah','atas','bawah','dpp'\n",
    "}\n",
    "# Show filtering:\n",
    "tokens = tokenize(sample_text)[:100]\n",
    "filtered = [t for t in tokens if t not in INDONESIAN_STOPWORDS]\n",
    "print(\"Before (sample):\", tokens[:30])\n",
    "print(\"After filtered (sample):\", filtered[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb16fe6",
   "metadata": {},
   "source": [
    "## Enhanced Confix Stripping (ECS) â€” principle\n",
    "ECS is rule-based: remove common Indonesian suffixes and prefixes (confixes) using a sequence of morphological rules (inflectional suffix stripping, derivational suffix stripping, prefix handling). For clarity we present a transparent implementation (note: production systems recommend using a lexicon check to verify roots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e67512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_particle_suffixes(word):\n",
    "    # remove enclitics / particles\n",
    "    return re.sub(r'(lah|kah|pun|nya|ku|mu)$', '', word)\n",
    "\n",
    "def remove_derivational_suffixes(word):\n",
    "    return re.sub(r'(i|kan|an)$', '', word)\n",
    "\n",
    "def remove_prefixes(word):\n",
    "    # simplified prefix handling following ECS-like rules with transformations\n",
    "    w = word\n",
    "    # handle meny- -> s + root (e.g., menyanyi -> nyanyi or s + vowel)\n",
    "    w = re.sub(r'^meny([aiueo].*)$', r's\\1', w)\n",
    "    w = re.sub(r'^men([aiueo].*)$', r'\\1', w)\n",
    "    w = re.sub(r'^mem([aiueo].*)$', r'p\\1', w)\n",
    "    w = re.sub(r'^meng([aiueo].*)$', r'\\1', w)\n",
    "    # simple removal for common prefixes\n",
    "    w = re.sub(r'^(di|ke|se|te|be|ber|pe|per|pel)', '', w)\n",
    "    return w\n",
    "\n",
    "def ecs_stem(word):\n",
    "    original = word\n",
    "    w = remove_particle_suffixes(word)\n",
    "    w = remove_derivational_suffixes(w)\n",
    "    w = remove_prefixes(w)\n",
    "    # final cleanup\n",
    "    w = re.sub(r'^(?:ku|kau)', '', w)  # possessives\n",
    "    if w == '':\n",
    "        w = original  # fallback to original to avoid empty\n",
    "    return w\n",
    "\n",
    "# Examples\n",
    "examples = [\"mengambil\",\"makanan\",\"bermain\",\"permainan\",\"membaca\",\"penyanyi\",\"menyanyi\",\"memakan\",\"pemakan\"]\n",
    "for ex in examples:\n",
    "    print(f\"{ex} -> {ecs_stem(ex)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3a7cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = tokenize(text)\n",
    "    filtered = [t for t in tokens if t not in INDONESIAN_STOPWORDS]\n",
    "    stems = [ecs_stem(t) for t in filtered]\n",
    "    return {\n",
    "        'tokens': tokens,\n",
    "        'filtered': filtered,\n",
    "        'stems': stems\n",
    "    }\n",
    "\n",
    "# Apply to all docs\n",
    "doc_preproc = {}\n",
    "for name, text in documents.items():\n",
    "    doc_preproc[name] = preprocess(text)\n",
    "\n",
    "# Show one document's pipeline\n",
    "name = sample_name\n",
    "print(\"Tokens (first 30):\", doc_preproc[name]['tokens'][:30])\n",
    "print(\"Filtered (first 30):\", doc_preproc[name]['filtered'][:30])\n",
    "print(\"Stems (first 30):\", doc_preproc[name]['stems'][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b826146",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted({stem for d in doc_preproc.values() for stem in d['stems']})\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(\"Sample vocab terms:\", vocab[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5cc874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF raw counts\n",
    "tf = pd.DataFrame(0, index=sorted(documents.keys()), columns=vocab)\n",
    "for doc, data in doc_preproc.items():\n",
    "    c = Counter(data['stems'])\n",
    "    for term, cnt in c.items():\n",
    "        tf.loc[doc, term] = cnt\n",
    "\n",
    "# Show table for first 10 terms and first 8 documents\n",
    "display(tf.iloc[:8, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bcdf60",
   "metadata": {},
   "source": [
    "IDF formula used: IDF(term) = log(N / df(term))  where N = number of documents, df = document frequency (docs containing term).\n",
    "We use natural log for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e8b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(tf)\n",
    "df = (tf > 0).sum(axis=0)\n",
    "idf = df.apply(lambda d: math.log(N / d) if d > 0 else 0)\n",
    "idf_df = pd.DataFrame({'df': df, 'idf': idf})\n",
    "display(idf_df.sort_values('df').head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9fd989",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tf.copy().astype(float)\n",
    "for term in tfidf.columns:\n",
    "    tfidf[term] = tfidf[term] * idf[term]\n",
    "# Show\n",
    "display(tfidf.iloc[:8, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e1bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"cara memasak ayam goreng\"  # sample query in Indonesian\n",
    "q_p = preprocess(query)\n",
    "print(\"Query tokens:\", q_p['tokens'])\n",
    "print(\"Query filtered:\", q_p['filtered'])\n",
    "print(\"Query stems:\", q_p['stems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21150101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stems(stems, vocab, idf_series):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    c = Counter(stems)\n",
    "    for i, term in enumerate(vocab):\n",
    "        vec[i] = c[term] * idf_series.get(term, 0.0)\n",
    "    return vec\n",
    "\n",
    "query_vec = vectorize_stems(q_p['stems'], vocab, idf)\n",
    "# Document vectors matrix\n",
    "doc_matrix = tfidf.values  # docs x terms\n",
    "\n",
    "# Cosine similarity\n",
    "def cosine_sim(v1, v2):\n",
    "    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "\n",
    "scores = {}\n",
    "for i, doc in enumerate(tfidf.index):\n",
    "    scores[doc] = cosine_sim(query_vec, doc_matrix[i])\n",
    "\n",
    "# Show values\n",
    "score_df = pd.DataFrame.from_dict(scores, orient='index', columns=['score']).sort_values('score', ascending=False)\n",
    "display(score_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc8270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results = score_df[score_df['score'] > 0].sort_values('score', ascending=False)\n",
    "print(\"Top ranked documents for query:\", query)\n",
    "display(top_results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c243f",
   "metadata": {},
   "source": [
    "## Conclusion & Observations\n",
    "- The pipeline shows step-by-step preprocessing: case folding, tokenization, stopword removal, ECS stemming.\n",
    "- TF-IDF with cosine similarity provides a ranked list of documents matching semantic word stems.\n",
    "- Limitations: Our ECS implementation is simplified and heuristic-based; production usage benefits from a lexical root-check or a more complete morphological analyzer.\n",
    "- Next steps: add query expansion, use word embeddings, and add evaluation (precision/recall) with labelled queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
